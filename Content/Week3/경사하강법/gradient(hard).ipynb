{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< 선형회귀분석 복습 > with 경사하강법 (Moore-Penrose 사용X)\n",
    "\n",
    "< 경사하강법으로 선형회귀 계수 구하기 >\n",
    "\n",
    "- 선형회귀의 목적식을 ||y-X(beta)||_2 이고, 이를 최소화하는 (beta)를 찾아야 함\n",
    "- nabla(||y-X(beta)||_2) : gradient 벡터를 구해야함\n",
    "- n개의 데이터를 가지고 사용되는 L2-norm이므로 일반 L2-norm과는 달리 제곱값을 다 더해준 뒤 n으로 나누고 루트를 씌워줌\n",
    "\n",
    "for t in range(T) :\n",
    "    error = y - x @ beta\n",
    "    grad = - transpose(X) @ error\n",
    "    beta -= lr * grad\n",
    "\n",
    "- 이제 경사하강법 알고리즘으로 역행렬을 이용하지 않고 회귀계수를 계산할 수 있음\n",
    "- 경사하강법 알고리즘에서는 학습률과 학습횟수가 중요한 hyperparameter가 된다\n",
    "\n",
    "< 경사하강법은 만능인가? >\n",
    "\n",
    "- 이론적으로 경사하강법은 미분가능하고 볼록(convex)한 함수에 대해선 적절한 학습률과 학습횟수를 선택했을 때 수렴이 보장되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f8929016d30f12a89eab5b5e02a6c1410fc73da4b23b89b4eb7a3bc58137fe5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
